{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80600fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be7ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the api keys\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4f139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "## hf login\n",
    "login(HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "## openai client\n",
    "openai_client=OpenAI(api_key=OPENAI_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240958a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = 'gpt-4o-mini'\n",
    "OPENSOURCE_MODEL = 'Qwen/CodeQwen1.5-7B-Chat'\n",
    "OPENSOURCE_ENDPOINT = 'https://qejgyemly6rdpbl2.us-east4.gcp.endpoints.huggingface.cloud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ac7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an assistant that creates doc strings in reStructure Text format for an existing python function. \n",
    "Respond only with an updated python function; use comments sparingly and do not provide any explanation other than occasional comments.\n",
    "Be sure to include typing annotation for each function argument or key word argument and return object types.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a36fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt(python_code):\n",
    "    user_prompt = \"\"\"\n",
    "        Rewrite this Python function with doc strings in the reStructuredText style.\n",
    "        Respond only with python code; do not explain your work other than a few comments.\n",
    "        Be sure to write a description of the function purpose with typing for each argument and return.\n",
    "        You are not allowed to change any part of the code.\n",
    "        Take a look at the following code and work accordingly-\n",
    "        \"\"\"\n",
    "    user_prompt+=python_code\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6385d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(python_code):\n",
    "    return [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\", \"content\": get_user_prompt(python_code)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0dd6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ = \"\"\"\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_2(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1adee7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(python_code):\n",
    "    output_stream = openai_client.chat.completions.create(\n",
    "        model = OPENAI_MODEL,\n",
    "        messages=get_messages(python_code),\n",
    "        stream=True\n",
    "    )\n",
    "    reply=\"\"\n",
    "    for chunk in output_stream:\n",
    "        text = chunk.choices[0].delta.content\n",
    "        reply+=text or \"\"\n",
    "        yield reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68cc9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configuring OPENSOURCE model\n",
    "def stream_open_source(python_code):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OPENSOURCE_MODEL)\n",
    "    messages = get_messages(python_code)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    client = InferenceClient(\n",
    "        OPENSOURCE_ENDPOINT,\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    stream = client.text_generation(\n",
    "        text,\n",
    "        stream=True,\n",
    "        details=True,\n",
    "        max_new_tokens=2000\n",
    "    )\n",
    "    result = \"\"\n",
    "    for r in stream:\n",
    "        result+=r.token.text or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28549680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model, python_code):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python_code)\n",
    "    else:\n",
    "        result = stream_open_source(python_code)\n",
    "    for r in result:\n",
    "        yield r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8055c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"# Docstring generator\")\n",
    "\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label='Code without docstring', value = python_, lines=10)\n",
    "        outcome = gr.Textbox(label=\"Code with docstring\", lines=15)\n",
    "\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"QWEN\"], label=\"Select a Model\", value=\"GPT\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        generate = gr.Button(\"Generate Docstring\")\n",
    "    generate.click(fn=optimize, inputs=[model,python], outputs=[outcome])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
