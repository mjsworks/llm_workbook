{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222675c1-81a9-482c-a938-876008b12835",
   "metadata": {},
   "source": [
    "## Research Paper Summarizer\n",
    "\n",
    "this is a notebook-native pipeline to fetch papers/articles and summarize them with llm.\n",
    "\n",
    "**Modes Supported**\n",
    "- Local files (txt, md, html, pdf)\n",
    "- Topic search (searches 3 relevant papers using arXiv API)\n",
    "- Paper title\n",
    "- Direct like\n",
    "\n",
    "**Output**\n",
    "- TL;DR, key points, entities/terms, one paragraph summary, questions\n",
    "- saves output/*.md + an index csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ba084-7519-4c32-af13-817fca2fcc54",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de97a25-bc4b-47d1-9c5a-dfdfe61e8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import requests\n",
    "from readability import Document\n",
    "import feedparser\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import fitz\n",
    "import tiktoken \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effbdaec-bdb7-4f41-81e5-0f9ef0a3881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output folder\n",
    "os.makedirs('outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495a654-5c2c-4ab5-8e7b-34581d4edc0b",
   "metadata": {},
   "source": [
    "### configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68882cd-2ba2-4197-aad7-247367af9e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system will use openai and the model is gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "with open(\"params.yaml\", \"r\") as f:\n",
    "    params=yaml.safe_load(f)\n",
    "\n",
    "cfg = params[\"config\"]\n",
    "USE_LLM=cfg[\"use_llm\"]\n",
    "USE_MODEL=cfg[\"use_model\"]\n",
    "MAX_TOKENS = int(cfg[\"max_tokens\"])\n",
    "OVERLAP_TOKENS = int(cfg[\"overlap_tokens\"])\n",
    "HEADERS = cfg[\"headers\"]\n",
    "MAX_ARXIV_RESULTS = int(cfg[\"max_arxiv_results\"])\n",
    "PAPER_SORT_STRATEGY = cfg[\"paper_sort_strategy\"]\n",
    "\n",
    "print(f\"The system will use {USE_LLM} and the model is {USE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a5528d-e966-4f5f-bc06-4ff7cb0b959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the openai api key\n",
    "def require_env():\n",
    "    load_dotenv()\n",
    "    v = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not v:\n",
    "        raise RuntimeError(\"Openai API Key not found\")\n",
    "    print(\"API KEY FOUND\")\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7ba14f-08ee-46f6-aee6-83dd42ccebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API KEY FOUND\n"
     ]
    }
   ],
   "source": [
    "api_key=require_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525534ce-64da-4029-b7d5-02152b7e933d",
   "metadata": {},
   "source": [
    "### utilities: token counting and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b8d26c-f4c7-45bf-afff-e9c6177ff435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- token helpers (robust) ---\n",
    "def count_tokens(text: str) -> int:\n",
    "    try:\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "        return len(enc.encode(text))\n",
    "    except Exception:\n",
    "        # fallback heuristic ~4 chars/token\n",
    "        return max(1, len(text) // 4)\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    max_tokens: int,\n",
    "    overlap_tokens: int\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into token-aware chunks with overlap.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks, current = [], []\n",
    "    token_count = 0\n",
    "    for w in words:\n",
    "        approx = max(1, len(w) // 4)  # heuristic to avoid calling tiktoken per word\n",
    "        if token_count + approx > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            # carry overlap (approximate chars ~= overlap_tokens*4)\n",
    "            overlap = \" \".join(current)[-overlap_tokens * 4:]\n",
    "            current = [overlap] if overlap else []\n",
    "            token_count = count_tokens(overlap) if overlap else 0\n",
    "        current.append(w)\n",
    "        token_count += approx\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef57fd9-2ff7-47a2-ba3a-3415177ecfff",
   "metadata": {},
   "source": [
    "### Content extraction - URL/HTML/PDF/LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf381056-31e8-40e9-8b3f-7a42ad2f27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = HEADERS\n",
    "\n",
    "# html\n",
    "def html_to_text(html: str) -> str:\n",
    "    doc = Document(html)\n",
    "    main = doc.summary()\n",
    "    soup = BeautifulSoup(main, \"html.parser\")\n",
    "    for irrelevant in soup([\"script\", \"style\", \"img\"]):\n",
    "        irrelevant.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# URL\n",
    "def fetch_url(url: str) -> str:\n",
    "    r = requests.get(url, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    ctype = (r.headers.get(\"Content-Type\", \"\") or \"\").lower()\n",
    "\n",
    "    if url.lower().endswith(\".pdf\") or \"application/pdf\" in ctype:\n",
    "        # open PDF from bytes\n",
    "        with fitz.open(stream=r.content, filetype=\"pdf\") as doc:\n",
    "            pages = [p.get_text() for p in doc]\n",
    "        return \"\\n\\n\".join(pages)\n",
    "    else:\n",
    "        return html_to_text(r.text)\n",
    "\n",
    "# local\n",
    "def load_local_text(path: str) -> str:\n",
    "    # PDFs\n",
    "    if path.lower().endswith(\".pdf\"):\n",
    "        with fitz.open(path) as doc:\n",
    "            pages = [p.get_text() for p in doc]\n",
    "        return \"\\n\\n\".join(pages)\n",
    "\n",
    "    # Read bytes then decode robustly\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    try:\n",
    "        s = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        s = raw.decode(\"latin-1\", errors=\"ignore\")\n",
    "\n",
    "    # HTML-ish?\n",
    "    if \"<html\" in s.lower() or \"<p\" in s.lower():\n",
    "        return html_to_text(s)\n",
    "\n",
    "    # Plain text cleanup\n",
    "    return re.sub(r\"\\n{2,}\", \"\\n\\n\", s).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4834ad-59fe-4846-968d-2ac15d135fff",
   "metadata": {},
   "source": [
    "### arXiv Search Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80da2373-e427-427c-a66f-4be71baf0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV_API = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "def arxiv_search(\n",
    "    query: str,\n",
    "    max_results: int,\n",
    "    sortBy: str\n",
    ") -> List[Dict]:\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results,\n",
    "        \"sortBy\": sortBy,\n",
    "        \"sortOrder\": \"descending\"\n",
    "    }\n",
    "    url = ARXIV_API + \"?\" + \"&\".join(f\"{k}={requests.utils.quote(str(v))}\" for k, v in params.items())\n",
    "    feed = feedparser.parse(url)\n",
    "    results = []\n",
    "    for entry in feed.entries:\n",
    "        pdf_link = next((l.href for l in entry.links if getattr(l, \"type\", \"\") == \"application/pdf\"), None)\n",
    "        results.append({\n",
    "            \"title\": entry.title,\n",
    "            \"summary\": entry.summary,\n",
    "            \"authors\": [a.name for a in getattr(entry, \"authors\", [])],\n",
    "            \"link\": entry.link,\n",
    "            \"pdf\": pdf_link,\n",
    "            \"published\": getattr(entry, \"published\", \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def build_text_from_arxiv_entry(entry: Dict) -> str:\n",
    "    parts = [\n",
    "        f\"Title: {entry.get('title','')}\",\n",
    "        f\"Authors: {', '.join(entry.get('authors', []))}\",\n",
    "        f\"Published: {entry.get('published','')}\",\n",
    "        \"\",\n",
    "        \"Abstract:\",\n",
    "        entry.get(\"summary\",\"\")\n",
    "    ]\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06472ae-ffd5-4b5c-af75-f9e0b83e3abe",
   "metadata": {},
   "source": [
    "### prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec4c46e-20b1-4eda-9936-1b2e74c76152",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a research assistant. Summarize the following text into structured sections:\n",
    "\n",
    "1) TL;DR (2–3 bullet points)\n",
    "2) Key Points (5–10 bullets)\n",
    "3) Entities & Terms (important names, orgs, dates, technical terms)\n",
    "4) One-paragraph Summary (5–7 sentences, neutral and precise)\n",
    "5) Questions to Explore (3–5 thoughtful questions)\n",
    "\n",
    "Be concise and faithful to the source. Avoid speculation.\n",
    "Text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9bbce34-2dc8-4eef-9ad0-e387cb514bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_params = params[\"prompt\"]\n",
    "TONE = prompt_params[\"tone\"]\n",
    "AUDIENCE = prompt_params[\"audience\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3cf669-1af6-4883-8f9b-6de19b0131db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(\n",
    "    text: str,\n",
    "    tone: str,\n",
    "    audience: str\n",
    ") -> str:\n",
    "    return f\"\"\"TONE: {tone}\n",
    "AUDIENCE: {audience}\n",
    "{system_prompt}\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed3102-918d-47fc-919a-4a93322baa2c",
   "metadata": {},
   "source": [
    "### LLM Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8251ed50-2c88-4c32-ba98-cab156cde566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def summarize_paper(\n",
    "    text: str,\n",
    "    tone: str,\n",
    "    audience: str,\n",
    "    model: str = USE_MODEL,\n",
    "    api_key: Optional[str] = None,\n",
    ") -> str:\n",
    "    api_key = api_key or os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set.\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    md_parts = []\n",
    "    chunks = chunk_text(text, max_tokens=MAX_TOKENS, overlap_tokens=OVERLAP_TOKENS)\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        prompt = build_prompt(chunk, tone=tone, audience=audience)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        md_parts.append(f\"## Chunk {i}\\n\\n{resp.choices[0].message.content}\")\n",
    "\n",
    "    return \"# Summary\\n\\n\" + \"\\n\\n---\\n\\n\".join(md_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756f5a8-022f-4d22-aa9b-53d97fe8ce50",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6b34cd-2c8b-4750-a087-807da83aab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save(md: str, source: str) -> str:\n",
    "    safe = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", source)[:80].strip(\"_\") or \"summary\"\n",
    "    path = f\"outputs/{safe}.md\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md)\n",
    "    return path\n",
    "\n",
    "def summarize_text_block(text: str, tone: str, audience: str) -> str:\n",
    "    return summarize_paper(text, tone=tone, audience=audience)\n",
    "\n",
    "def run_local_files(paths: List[str], tone: str, audience: str) -> List[Dict]:\n",
    "    out = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            txt = load_local_text(p)\n",
    "            md = summarize_text_block(txt, tone, audience)\n",
    "            md_path = merge_and_save(md, f\"file_{os.path.basename(p)}\")\n",
    "            out.append({\"mode\":\"local\", \"source\": p, \"markdown_path\": md_path})\n",
    "        except Exception as e:\n",
    "            out.append({\"mode\":\"local\", \"source\": p, \"error\": str(e)})\n",
    "    return out\n",
    "\n",
    "def run_topic_search(topic: str, tone: str, audience: str, top_n: int = MAX_ARXIV_RESULTS) -> List[Dict]:\n",
    "    entries = arxiv_search(topic, max_results=top_n, sortBy=PAPER_SORT_STRATEGY)\n",
    "    out = []\n",
    "    for e in entries:\n",
    "        try:\n",
    "            txt = build_text_from_arxiv_entry(e)\n",
    "            md = summarize_text_block(txt, tone, audience)\n",
    "            md_path = merge_and_save(md, f\"topic_{topic}_{e['title']}\")\n",
    "            out.append({\"mode\":\"topic\", \"source\": e[\"link\"], \"title\": e[\"title\"], \"markdown_path\": md_path})\n",
    "        except Exception as ex:\n",
    "            out.append({\"mode\":\"topic\", \"source\": e.get(\"link\",\"\"), \"title\": e.get(\"title\",\"\"), \"error\": str(ex)})\n",
    "    return out\n",
    "\n",
    "def run_paper_title(title: str, tone: str, audience: str) -> List[Dict]:\n",
    "    entries = arxiv_search(title, max_results=1, sortBy=PAPER_SORT_STRATEGY)\n",
    "    if not entries:\n",
    "        return [{\"mode\":\"title\", \"source\": title, \"error\":\"No results found on arXiv\"}]\n",
    "    e = entries[0]\n",
    "    try:\n",
    "        txt = build_text_from_arxiv_entry(e)\n",
    "        md = summarize_text_block(txt, tone, audience)\n",
    "        md_path = merge_and_save(md, f\"title_{e['title']}\")\n",
    "        return [{\"mode\":\"title\", \"source\": e[\"link\"], \"title\": e[\"title\"], \"markdown_path\": md_path}]\n",
    "    except Exception as ex:\n",
    "        return [{\"mode\":\"title\", \"source\": e.get(\"link\",\"\"), \"title\": e.get(\"title\",\"\"), \"error\": str(ex)}]\n",
    "\n",
    "def run_direct_links(links: List[str], tone: str, audience: str) -> List[Dict]:\n",
    "    out = []\n",
    "    for url in links:\n",
    "        try:\n",
    "            txt = fetch_url(url)\n",
    "            md = summarize_text_block(txt, tone, audience)\n",
    "            md_path = merge_and_save(md, f\"url_{url}\")\n",
    "            out.append({\"mode\":\"link\", \"source\": url, \"markdown_path\": md_path})\n",
    "        except Exception as e:\n",
    "            out.append({\"mode\":\"link\", \"source\": url, \"error\": str(e)})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb236c00-c010-41f5-a77a-78a7f248d2a6",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2913816c-f7e1-46a6-b7bb-17e5bbf7cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = run_direct_links([\"https://arxiv.org/pdf/2010.11929.pdf\"], tone=TONE, audience=AUDIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9f983f-0440-40d1-93cc-b48ad61f5d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>source</th>\n",
       "      <th>markdown_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>link</td>\n",
       "      <td>https://arxiv.org/pdf/2010.11929.pdf</td>\n",
       "      <td>outputs/url_https_arxiv_org_pdf_2010_11929_pdf.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mode                                source  \\\n",
       "0  link  https://arxiv.org/pdf/2010.11929.pdf   \n",
       "\n",
       "                                       markdown_path  \n",
       "0  outputs/url_https_arxiv_org_pdf_2010_11929_pdf.md  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rows).to_csv(\"outputs/summaries_index.csv\", index=False)\n",
    "pd.DataFrame(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
