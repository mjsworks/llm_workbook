## Context Window
the total number of tokens that an LLM can examine at any one point when it is trying to generate the next token.

So, any word that has been generated by the LLMs are generated based on a limited number of inputs.

when we use some Web Interface like chatGPT, it appears that it remembers the things we have discussed before. this is bit of an illusion or a trick.

so it does not remember things. as we move forward in the chat, the entire messages and response generated by the model is passed into the model again as a form of a long input prompt. and the model generates - "what is most likely to come next given all the conversation so far."

so when we start a conversation, the model only keeps up with the starting part - it is easy. as it moves forward, it has to fit all the consecutive messages and responses to fit into the chat to keep the context.